h2. Description

This template will return descriptive statistics of numerical or frequency tables of categorical variables.

h2. _gender_ (&quot;Gender&quot;)

The dataset has _709_ observations with _673_ valid values (missing: _36_) in _gender_ (&quot;Gender&quot;). This variable seems to be a factor.

h3. Base statistics

<!-- endlist -->

|_. |_. *gender*|_. *N*|_. *pct*|_. *cumul.count*|_. *cumul.pct*|
|1|male|410|60.9212|410|60.9212|
|2|female|263|39.0788|673|100|
|Total||673|100|673|100|


h3. Barplot

!3ed92ab3ffc6e875335e7e8c774c35a8.png!


It seems that the highest value is _2_ which is exactly 2 times higher than the smallest value (_1_).

h2. _age_ (&quot;Age&quot;)

The dataset has _709_ observations with _677_ valid values (missing: _32_) in _age_ (&quot;Age&quot;). This variable seems to be numeric.

h3. Base statistics

|_. *value*|_. *mean(age)*|_. *sd(age)*|_. *var(age)*|
|(all)|24.5731|6.8491|46.9107|


h3. Histogram

!ac5d789145bdef09b10219ef16429f53.png!


It seems that the highest value is _58_ which is exactly 3.625 times higher than the smallest value (_16_).

The standard deviation is 6.8491 (variance: 46.9107). The expected value is around 24.5731, somewhere between 24.0572 and 25.0891 (SE: 0.2632).

If we suppose that _Age_ is not near to a normal distribution (test: see below, skewness: 1.9296, kurtosis: 7.4851), checking the median (23) might be a better option instead of the mean. The interquartile range (6) measures the statistics dispersion of the variable (similar to standard deviation) based on median.

h3. Normality tests

h2. Introduction

In statistics, _normality_ refers to an assumption that the distribution of a random variable follows _normal_ (_Gaussian_) distribution. Because of its bell-like shape, it's also known as the _&quot;bell curve&quot;_. The formula for _normal distribution_ is:

<span class="math">f(x) = \frac{1}{\sqrt{2\pi{}\sigma{}^2}} e^{-\frac{(x-\mu{})^2}{2\sigma{}^2}}</math>

_Normal distribution_ belongs to a _location-scale family_ of distributions, as it's defined two parameters:

* _μ_ - _mean_ or _expectation_ (location parameter)
* _σ[^2^]_ - _variance_ (scale parameter)

!2f8c434e103f36ec70966b372838d448.png!


h2. Normality Tests

h3. Overview

Various hypothesis tests can be applied in order to test if the distribution of given random variable violates normality assumption. These procedures test the H[~0~] that provided variable's distribution is _normal_. At this point only few such tests will be covered: the ones that are available in @stats@ package (which comes bundled with default R installation) and @nortest@ package that is "available":http://cran.r-project.org/web/packages/nortest/index.html on CRAN.

* *Shapiro-Wilk test* is a powerful normality test appropriate for small samples. In R, it's implemented in @shapiro.test@ function available in @stats@ package.
* *Lilliefors test* is a modification of _Kolmogorov-Smirnov test_ appropriate for testing normality when parameters or normal distribution (_μ_, _σ[^2^]_) are not known. @lillie.test@ function is located in @nortest@ package.
* *Anderson-Darling test* is one of the most powerful normality tests as it will detect the most of departures from normality. You can find @ad.test@ function in @nortest@ package.
* *Pearson Χ[^2^] test* is another normality test which takes more &quot;traditional&quot; approach in normality testing. @pearson.test@ is located in @nortest@ package.

h3. Results

Here you can see the results of applied normality tests (_p-values_ less than 0.05 indicate significant discrepancies):

<!-- endlist -->

|_. |_. *H*|_. *p*|
|shapiro.test|0.8216|0|
|lillie.test|0.17|0|
|ad.test|32.1645|0|
|pearson.test|625.8479|0|


So, let's draw some conclusions based on applied normality test:

* according to _Shapiro-Wilk test_, the distribution of _Age_ is not normal.
* based on _Lilliefors test_, distribution of _Age_ is not normal
* _Anderson-Darling test_ confirms violation of normality assumption
* _Pearson's Χ[^2^] test_ classifies the underlying distribution as non-normal

h2. Diagnostic Plots

There are various plots that can help you decide about the normality of the distribution. Only a few most commonly used plots will be shown: _histogram_, _Q-Q plot_ and _kernel density plot_.

h3. Histogram

_Histogram_ was first introduced by _Karl Pearson_ and it's probably the most popular plot for depicting the probability distribution of a random variable. However, the decision depends on number of bins, so it can sometimes be misleading. If the variable distribution is normal, bins should resemble the &quot;bell-like&quot; shape.

!ac5d789145bdef09b10219ef16429f53.png!


h3. Q-Q Plot

&quot;Q&quot; in _Q-Q plot_ stands for _quantile_, as this plot compares empirical and theoretical distribution (in this case, _normal_ distribution) by plotting their quantiles against each other. For normal distribution, plotted dots should approximate a &quot;straight&quot;, @x = y@ line.

!cbbba756d844aa053998959b73b9feff.png!


h3. Kernel Density Plot

_Kernel density plot_ is a plot of smoothed _empirical distribution function_. As such, it provides good insight about the shape of the distribution. For normal distributions, it should resemble the well known &quot;bell shape&quot;.

!6fd0494eea748495baa80653752f194f.png!


h2. Description

This template will return descriptive statistics of numerical or frequency tables of categorical variables.

h2. _chatim_ (&quot;Chat &amp; IM usage&quot;)

The dataset has _709_ observations with _669_ valid values (missing: _40_) in _chatim_ (&quot;Chat &amp; IM usage&quot;). This variable seems to be a factor.

h3. Base statistics

<!-- endlist -->

|_. |_. *chatim*|_. *N*|_. *pct*|_. *cumul.count*|_. *cumul.pct*|
|1|never|60|8.9686|60|8.9686|
|2|very rarely|73|10.9118|133|19.8804|
|3|rarely|58|8.6697|191|28.5501|
|4|sometimes|113|16.8909|304|45.441|
|5|often|136|20.3288|440|65.7698|
|6|very often|88|13.154|528|78.9238|
|7|always|141|21.0762|669|100|
|Total||669|100|669|100|


h3. Barplot

!5a00abbe4c793ceedbbf10939665b5cf.png!


It seems that the highest value is _7_ which is exactly 7 times higher than the smallest value (_1_).

h2. _game_ (&quot;On-line games usage&quot;)

The dataset has _709_ observations with _677_ valid values (missing: _32_) in _game_ (&quot;On-line games usage&quot;). This variable seems to be a factor.

h3. Base statistics

<!-- endlist -->

|_. |_. *game*|_. *N*|_. *pct*|_. *cumul.count*|_. *cumul.pct*|
|1|never|352|51.9941|352|51.9941|
|2|very rarely|128|18.9069|480|70.901|
|3|rarely|32|4.7267|512|75.6278|
|4|sometimes|60|8.8626|572|84.4904|
|5|often|37|5.4653|609|89.9557|
|6|very often|35|5.1699|644|95.1256|
|7|always|33|4.8744|677|100|
|Total||677|100|677|100|


h3. Barplot

!e53046a09491443064e085131e547971.png!


It seems that the highest value is _7_ which is exactly 7 times higher than the smallest value (_1_).

h2. _surf_ (&quot;Web surfing usage&quot;)

The dataset has _709_ observations with _678_ valid values (missing: _31_) in _surf_ (&quot;Web surfing usage&quot;). This variable seems to be a factor.

h3. Base statistics

<!-- endlist -->

|_. |_. *surf*|_. *N*|_. *pct*|_. *cumul.count*|_. *cumul.pct*|
|1|never|17|2.5074|17|2.5074|
|2|very rarely|26|3.8348|43|6.3422|
|3|rarely|33|4.8673|76|11.2094|
|4|sometimes|107|15.7817|183|26.9912|
|5|often|158|23.3038|341|50.295|
|6|very often|142|20.944|483|71.2389|
|7|always|195|28.7611|678|100|
|Total||678|100|678|100|


h3. Barplot

!0166a8b5df2f3db871e8736bfee8af6e.png!


It seems that the highest value is _7_ which is exactly 7 times higher than the smallest value (_1_).

h2. _email_ (&quot;Email usage&quot;)

The dataset has _709_ observations with _672_ valid values (missing: _37_) in _email_ (&quot;Email usage&quot;). This variable seems to be a factor.

h3. Base statistics

<!-- endlist -->

|_. |_. *email*|_. *N*|_. *pct*|_. *cumul.count*|_. *cumul.pct*|
|1|never|13|1.9345|13|1.9345|
|2|very rarely|36|5.3571|49|7.2917|
|3|rarely|46|6.8452|95|14.1369|
|4|sometimes|87|12.9464|182|27.0833|
|5|often|123|18.3036|305|45.3869|
|6|very often|108|16.0714|413|61.4583|
|7|always|259|38.5417|672|100|
|Total||672|100|672|100|


h3. Barplot

!895cde198b269bf65b01e1e067a515c8.png!


It seems that the highest value is _7_ which is exactly 7 times higher than the smallest value (_1_).

h2. _download_ (&quot;Download usage&quot;)

The dataset has _709_ observations with _677_ valid values (missing: _32_) in _download_ (&quot;Download usage&quot;). This variable seems to be a factor.

h3. Base statistics

<!-- endlist -->

|_. |_. *download*|_. *N*|_. *pct*|_. *cumul.count*|_. *cumul.pct*|
|1|never|11|1.6248|11|1.6248|
|2|very rarely|28|4.1359|39|5.7607|
|3|rarely|29|4.2836|68|10.0443|
|4|sometimes|80|11.8168|148|21.8612|
|5|often|124|18.3161|272|40.1773|
|6|very often|160|23.6337|432|63.8109|
|7|always|245|36.1891|677|100|
|Total||677|100|677|100|


h3. Barplot

!dde181184885b8777d0248b3f421289a.png!


It seems that the highest value is _7_ which is exactly 7 times higher than the smallest value (_1_).

h2. _forum_ (&quot;Web forums usage&quot;)

The dataset has _709_ observations with _673_ valid values (missing: _36_) in _forum_ (&quot;Web forums usage&quot;). This variable seems to be a factor.

h3. Base statistics

<!-- endlist -->

|_. |_. *forum*|_. *N*|_. *pct*|_. *cumul.count*|_. *cumul.pct*|
|1|never|76|11.2927|76|11.2927|
|2|very rarely|80|11.8871|156|23.1798|
|3|rarely|72|10.6984|228|33.8782|
|4|sometimes|111|16.4933|339|50.3715|
|5|often|109|16.1961|448|66.5676|
|6|very often|119|17.682|567|84.2496|
|7|always|106|15.7504|673|100|
|Total||673|100|673|100|


h3. Barplot

!ac419134b2f4695e544d8886ba12e0c2.png!


It seems that the highest value is _7_ which is exactly 7 times higher than the smallest value (_1_).

h2. _socnet_ (&quot;Social networks usage&quot;)

The dataset has _709_ observations with _678_ valid values (missing: _31_) in _socnet_ (&quot;Social networks usage&quot;). This variable seems to be a factor.

h3. Base statistics

<!-- endlist -->

|_. |_. *socnet*|_. *N*|_. *pct*|_. *cumul.count*|_. *cumul.pct*|
|1|never|208|30.6785|208|30.6785|
|2|very rarely|102|15.0442|310|45.7227|
|3|rarely|57|8.4071|367|54.1298|
|4|sometimes|87|12.8319|454|66.9617|
|5|often|79|11.6519|533|78.6136|
|6|very often|80|11.7994|613|90.413|
|7|always|65|9.587|678|100|
|Total||678|100|678|100|


h3. Barplot

!8475d98870c1cdd2436a3abdb0d69a66.png!


It seems that the highest value is _7_ which is exactly 7 times higher than the smallest value (_1_).

h2. _xxx_ (&quot;Adult sites usage&quot;)

The dataset has _709_ observations with _674_ valid values (missing: _35_) in _xxx_ (&quot;Adult sites usage&quot;). This variable seems to be a factor.

h3. Base statistics

<!-- endlist -->

|_. |_. *xxx*|_. *N*|_. *pct*|_. *cumul.count*|_. *cumul.pct*|
|1|never|274|40.6528|274|40.6528|
|2|very rarely|124|18.3976|398|59.0504|
|3|rarely|52|7.7151|450|66.7656|
|4|sometimes|131|19.4362|581|86.2018|
|5|often|46|6.8249|627|93.0267|
|6|very often|28|4.1543|655|97.181|
|7|always|19|2.819|674|100|
|Total||674|100|674|100|


h3. Barplot

!4fda8cf992e8de93624c45ef3c72a0c5.png!


It seems that the highest value is _7_ which is exactly 7 times higher than the smallest value (_1_).

h2. Description

This template will return descriptive statistics of numerical or frequency tables of categorical variables.

h2. _hp_

The dataset has _32_ observations with _32_ valid values (missing: _0_) in _hp_. This variable seems to be numeric.

h3. Base statistics

|_. *value*|_. *mean(hp)*|_. *sd(hp)*|_. *var(hp)*|
|(all)|146.6875|68.5629|4700.8669|


h3. Histogram

!d90ec4a0af55fabeae7988710a062ce0.png!


It seems that the highest value is _335_ which is exactly 6.4423 times higher than the smallest value (_52_).

The standard deviation is 68.5629 (variance: 4700.8669). The expected value is around 146.6875, somewhere between 122.9317 and 170.4433 (SE: 12.1203).

If we suppose that _hp_ is not near to a normal distribution (test: see below, skewness: 0.7614, kurtosis: 3.0522), checking the median (123) might be a better option instead of the mean. The interquartile range (83.5) measures the statistics dispersion of the variable (similar to standard deviation) based on median.

h3. Normality tests

h2. Introduction

In statistics, _normality_ refers to an assumption that the distribution of a random variable follows _normal_ (_Gaussian_) distribution. Because of its bell-like shape, it's also known as the _&quot;bell curve&quot;_. The formula for _normal distribution_ is:

<span class="math">f(x) = \frac{1}{\sqrt{2\pi{}\sigma{}^2}} e^{-\frac{(x-\mu{})^2}{2\sigma{}^2}}</math>

_Normal distribution_ belongs to a _location-scale family_ of distributions, as it's defined two parameters:

* _μ_ - _mean_ or _expectation_ (location parameter)
* _σ[^2^]_ - _variance_ (scale parameter)

!2f8c434e103f36ec70966b372838d448.png!


h2. Normality Tests

h3. Overview

Various hypothesis tests can be applied in order to test if the distribution of given random variable violates normality assumption. These procedures test the H[~0~] that provided variable's distribution is _normal_. At this point only few such tests will be covered: the ones that are available in @stats@ package (which comes bundled with default R installation) and @nortest@ package that is "available":http://cran.r-project.org/web/packages/nortest/index.html on CRAN.

* *Shapiro-Wilk test* is a powerful normality test appropriate for small samples. In R, it's implemented in @shapiro.test@ function available in @stats@ package.
* *Lilliefors test* is a modification of _Kolmogorov-Smirnov test_ appropriate for testing normality when parameters or normal distribution (_μ_, _σ[^2^]_) are not known. @lillie.test@ function is located in @nortest@ package.
* *Anderson-Darling test* is one of the most powerful normality tests as it will detect the most of departures from normality. You can find @ad.test@ function in @nortest@ package.
* *Pearson Χ[^2^] test* is another normality test which takes more &quot;traditional&quot; approach in normality testing. @pearson.test@ is located in @nortest@ package.

h3. Results

Here you can see the results of applied normality tests (_p-values_ less than 0.05 indicate significant discrepancies):

<!-- endlist -->

|_. |_. *H*|_. *p*|
|shapiro.test|0.9334|0.0488|
|lillie.test|0.1664|0.0245|
|ad.test|0.7077|0.0584|
|pearson.test|11.5|0.0423|


So, let's draw some conclusions based on applied normality test:

* according to _Shapiro-Wilk test_, the distribution of _hp_ is not normal.
* based on _Lilliefors test_, distribution of _hp_ is not normal
* _Anderson-Darling test_ confirms normality assumption
* _Pearson's Χ[^2^] test_ classifies the underlying distribution as non-normal

h2. Diagnostic Plots

There are various plots that can help you decide about the normality of the distribution. Only a few most commonly used plots will be shown: _histogram_, _Q-Q plot_ and _kernel density plot_.

h3. Histogram

_Histogram_ was first introduced by _Karl Pearson_ and it's probably the most popular plot for depicting the probability distribution of a random variable. However, the decision depends on number of bins, so it can sometimes be misleading. If the variable distribution is normal, bins should resemble the &quot;bell-like&quot; shape.

!d90ec4a0af55fabeae7988710a062ce0.png!


h3. Q-Q Plot

&quot;Q&quot; in _Q-Q plot_ stands for _quantile_, as this plot compares empirical and theoretical distribution (in this case, _normal_ distribution) by plotting their quantiles against each other. For normal distribution, plotted dots should approximate a &quot;straight&quot;, @x = y@ line.

!17e5c77b83c6e3e636487406decc14c7.png!


h3. Kernel Density Plot

_Kernel density plot_ is a plot of smoothed _empirical distribution function_. As such, it provides good insight about the shape of the distribution. For normal distributions, it should resemble the well known &quot;bell shape&quot;.

!135de2b4d3cb1b1a3ece741c584c0a59.png!


h2. _wt_

The dataset has _32_ observations with _32_ valid values (missing: _0_) in _wt_. This variable seems to be numeric.

h3. Base statistics

|_. *value*|_. *mean(wt)*|_. *sd(wt)*|_. *var(wt)*|
|(all)|3.2172|0.9785|0.9574|


h3. Histogram

!10caa8222b28328a6d8fd28917cbfb45.png!


It seems that the highest value is _5.424_ which is exactly 3.5849 times higher than the smallest value (_1.513_).

The standard deviation is 0.9785 (variance: 0.9574). The expected value is around 3.2172, somewhere between 2.8782 and 3.5563 (SE: 0.173).

If we suppose that _wt_ is not near to a normal distribution (test: see below, skewness: 0.4438, kurtosis: 3.1725), checking the median (3.325) might be a better option instead of the mean. The interquartile range (1.0288) measures the statistics dispersion of the variable (similar to standard deviation) based on median.

h3. Normality tests

h2. Introduction

In statistics, _normality_ refers to an assumption that the distribution of a random variable follows _normal_ (_Gaussian_) distribution. Because of its bell-like shape, it's also known as the _&quot;bell curve&quot;_. The formula for _normal distribution_ is:

<span class="math">f(x) = \frac{1}{\sqrt{2\pi{}\sigma{}^2}} e^{-\frac{(x-\mu{})^2}{2\sigma{}^2}}</math>

_Normal distribution_ belongs to a _location-scale family_ of distributions, as it's defined two parameters:

* _μ_ - _mean_ or _expectation_ (location parameter)
* _σ[^2^]_ - _variance_ (scale parameter)

!2f8c434e103f36ec70966b372838d448.png!


h2. Normality Tests

h3. Overview

Various hypothesis tests can be applied in order to test if the distribution of given random variable violates normality assumption. These procedures test the H[~0~] that provided variable's distribution is _normal_. At this point only few such tests will be covered: the ones that are available in @stats@ package (which comes bundled with default R installation) and @nortest@ package that is "available":http://cran.r-project.org/web/packages/nortest/index.html on CRAN.

* *Shapiro-Wilk test* is a powerful normality test appropriate for small samples. In R, it's implemented in @shapiro.test@ function available in @stats@ package.
* *Lilliefors test* is a modification of _Kolmogorov-Smirnov test_ appropriate for testing normality when parameters or normal distribution (_μ_, _σ[^2^]_) are not known. @lillie.test@ function is located in @nortest@ package.
* *Anderson-Darling test* is one of the most powerful normality tests as it will detect the most of departures from normality. You can find @ad.test@ function in @nortest@ package.
* *Pearson Χ[^2^] test* is another normality test which takes more &quot;traditional&quot; approach in normality testing. @pearson.test@ is located in @nortest@ package.

h3. Results

Here you can see the results of applied normality tests (_p-values_ less than 0.05 indicate significant discrepancies):

<!-- endlist -->

|_. |_. *H*|_. *p*|
|shapiro.test|0.9433|0.0927|
|lillie.test|0.1356|0.1412|
|ad.test|0.6091|0.1038|
|pearson.test|4.5|0.4799|


So, let's draw some conclusions based on applied normality test:

* according to _Shapiro-Wilk test_, the distribution of _wt_ is normal.
* based on _Lilliefors test_, distribution of _wt_ is not normal
* _Anderson-Darling test_ confirms normality assumption
* _Pearson's Χ[^2^] test_ classifies the underlying distribution as non-normal

h2. Diagnostic Plots

There are various plots that can help you decide about the normality of the distribution. Only a few most commonly used plots will be shown: _histogram_, _Q-Q plot_ and _kernel density plot_.

h3. Histogram

_Histogram_ was first introduced by _Karl Pearson_ and it's probably the most popular plot for depicting the probability distribution of a random variable. However, the decision depends on number of bins, so it can sometimes be misleading. If the variable distribution is normal, bins should resemble the &quot;bell-like&quot; shape.

!10caa8222b28328a6d8fd28917cbfb45.png!


h3. Q-Q Plot

&quot;Q&quot; in _Q-Q plot_ stands for _quantile_, as this plot compares empirical and theoretical distribution (in this case, _normal_ distribution) by plotting their quantiles against each other. For normal distribution, plotted dots should approximate a &quot;straight&quot;, @x = y@ line.

!ff471a5bcb80aaf91b4c053ab038d69a.png!


h3. Kernel Density Plot

_Kernel density plot_ is a plot of smoothed _empirical distribution function_. As such, it provides good insight about the shape of the distribution. For normal distributions, it should resemble the well known &quot;bell shape&quot;.

!16a7d5cf96ceceffd6db59f9a2514dce.png!


<hr />

This report was generated with "rapport":http://rapport-package.info/.

!images/rapport.png!

